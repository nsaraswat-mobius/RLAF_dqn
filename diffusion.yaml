name: Train Text-to-Image LoRA Diffusion Model
description: Fine-tunes a LoRA diffusion model using provided model name, dataset name, and configuration.
inputs:
  - {name: model_name, type: String}
  - {name: dataset_name, type: String}
  - {name: config, type: String}
  - {name: hub_model_id, type: String, default: ""}
  - {name: hf_token, type: String, default: ""}
outputs:
  - {name: saved_model, type: Directory}
implementation:
  container:
    image: kushagra4761/diffusers-text-to-image:latest
    command:
      - sh
      - -c
      - |
        set -e
        echo "Starting pipeline execution."
        echo "Inputs received: model_name=$0, dataset_name=$1, config=$2, hub_model_id=$4"
        
        echo "Checking for GPU availability..."
        if command -v nvidia-smi >/dev/null 2>&1; then
          echo "GPU detected. Details:"
          nvidia-smi
        else
          echo "GPU detection utility 'nvidia-smi' not found or GPU not available."
        fi
        
        # Arguments: $0=model_name, $1=dataset_name, $2=config, $3=saved_model_path, $4=hub_model_id, $5=hf_token
        export MODEL_NAME="$0"
        export DATASET_NAME="$1"
        export OUTPUT_DIR="$3"
        export HUB_MODEL_ID="$4"
        export HF_TOKEN="$5"

        if [ -n "$HF_TOKEN" ]; then
          echo "Authenticating with Hugging Face Hub..."
          huggingface-cli login --token "$HF_TOKEN" --add-to-git-credential
        fi
        
        echo "Parsing configuration using input: $2"
        
        # Parse config ($2) and export training parameters
        # FIXED: Swapped internal single quotes for double quotes to prevent shell nesting errors
        eval "$(python3 -c '
        import json
        import sys
        import os

        try:
            config = json.loads(sys.argv[1])
        except (json.JSONDecodeError, IndexError):
            config = {}

        DEFAULTS = {
            "resolution": 512,
            "train_batch_size": 1,
            "num_train_epochs": 100,
            "checkpointing_steps": 5000,
            "learning_rate": 1e-04,
            "lr_scheduler": "constant",
            "lr_warmup_steps": 0,
            "seed": 42
        }

        settings = DEFAULTS.copy()
        settings.update(config)

        for key, value in settings.items():
            if isinstance(value, str):
                print(f"export {key.upper()}='{value}'")
            else:
                print(f"export {key.upper()}={value}")
        ' "$2")"
        set --
        echo "Exported parameters confirmed: RESOLUTION=$RESOLUTION, LEARNING_RATE=$LEARNING_RATE"

        accelerate launch --mixed_precision="no" train_text_to_image_lora.py \
            --pretrained_model_name_or_path="$MODEL_NAME" \
            --dataset_name="$DATASET_NAME" --caption_column="text" \
            --resolution="$RESOLUTION" --random_flip \
            --train_batch_size="$TRAIN_BATCH_SIZE" \
            --num_train_epochs="$NUM_TRAIN_EPOCHS" --checkpointing_steps="$CHECKPOINTING_STEPS" \
            --learning_rate="$LEARNING_RATE" --lr_scheduler="$LR_SCHEDULER" --lr_warmup_steps="$LR_WARMUP_STEPS" \
            --seed="$SEED" \
            --output_dir="$OUTPUT_DIR" \
            --validation_prompt="cute dragon creature" --report_to="wandb" \
            ${HUB_MODEL_ID:+--push_to_hub} \
            ${HUB_MODEL_ID:+--hub_model_id="$HUB_MODEL_ID"}
    args:
      - {inputValue: model_name}
      - {inputValue: dataset_name}
      - {inputValue: config}
      - {outputPath: saved_model}
      - {inputValue: hub_model_id}
      - {inputValue: hf_token}
